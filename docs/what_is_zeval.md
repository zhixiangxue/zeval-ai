# 什么是 Zeval？

## 📖 一句话介绍

**Zeval 是一个自动化的 AI 问答系统评估工具**，它能帮你测试和改进你的智能客服、知识问答机器人等 AI 系统。

## 🤔 为什么需要 Zeval？

### 问题场景

假设你的公司开发了一个智能客服机器人，用来回答客户关于产品的问题。你想知道：

- ❓ 机器人回答的准确吗？
- ❓ 它会不会"胡说八道"（即"幻觉"）？
- ❓ 它找到的资料相关吗？
- ❓ 回答是不是切题？

传统做法是：雇人手动问几百个问题，然后一个个检查答案。这样做：

- ⏰ **很慢**：一个人一天可能只能测试 20-30 个问题
- 💰 **很贵**：需要专业人员长期投入
- 😓 **很累**：重复性劳动，容易出错
- 📉 **不全面**：很难覆盖所有场景

### Zeval 的解决方案

Zeval 把这个过程**全自动化**了：

1. 📚 **自动出题**：从你的知识库（产品手册、FAQ 等）自动生成测试题
2. 🤖 **自动测试**：让机器人回答这些问题
3. 📊 **自动评分**：用 AI 评判答案的质量
4. 📄 **自动报告**：生成一份详细的体检报告，告诉你哪里有问题

**结果**：原本需要一周的工作，现在 1 小时就能完成。

---

## 🎯 Zeval 是如何工作的？

可以把 Zeval 想象成一个**自动化的质检流水线**，分为以下几个步骤：

### 第一步：读取知识库 📚

就像老师出题前要先读教材，Zeval 首先读取你的知识库文件：

- PDF 文档（如产品手册）
- Word 文档
- 网页内容
- Markdown 文件

**比如**：一份《住房贷款产品说明书》

### 第二步：理解内容 🧠

Zeval 会把大文档切分成小段落，并提取关键信息：

- **摘要**："这段讲的是首付比例要求"
- **关键词**："首付、信用评分、贷款额度"
- **重要术语**："FHA 贷款、DTI 比率"

**就像**：你在读书时划重点和做笔记。

### 第三步：设定角色 👥

为了让测试题更真实，Zeval 会创建不同的"用户角色"：

**例如**（住房贷款场景）：

- **小白用户**：首次购房者，对贷款流程不熟悉
  - 特点：信用分 680，预算 30-50 万美元，首付 10%
  - 会问："什么是 DTI 比率？"
  
- **专业用户**：房地产投资人，了解贷款产品
  - 特点：信用分 780，预算 100-200 万美元，首付 25%
  - 会问："如何优化 DTI 比率以提高贷款额度？"

- **特殊需求用户**：退伍军人，关注 VA 贷款
  - 特点：信用分 720，预算 40-70 万美元
  - 会问："VA 贷款和 FHA 贷款的优势对比？"

**作用**：确保测试题覆盖不同用户的真实需求。

### 第四步：自动出题 ✍️

根据角色和知识库内容，自动生成测试题：

**生成过程**：

1. 选择一个角色：小白用户
2. 选择一个主题：首付要求
3. 根据知识库生成问题和标准答案，如事实性的问题（单跳）、推理性的问题（多跳）

**示例题目**：

```
问题：我信用分 680，想买 40 万的房子，最低需要多少首付？

标准答案：根据 FHA 贷款政策，信用分在 580-679 之间，
最低首付比例为 10%（即 4 万美元）。如果信用分提高到 
680 以上，首付可降低至 3.5%（即 1.4 万美元）。
```

**智能约束**：

- ✅ 每个主题不会重复出太多题（避免"什么是 DTI？"出现 10 次）
- ✅ 各角色的题目数量均衡
- ✅ 答案必须能从知识库中找到依据

### 第五步：让机器人答题 🤖

把生成的问题发给你的 AI 机器人，记录下：

- **机器人的回答**
- **机器人找到的参考资料**（它是从哪些文档片段找的答案）

### 第六步：多维度评分 📊

Zeval 用 6 个维度给机器人打分，就像体检报告的各项指标：

#### 🔍 检索能力评估（机器人找资料的能力）

| 指标 | 通俗解释 | 评分标准 |
|------|----------|----------|
| **相关性** | 找到的资料是否和问题有关？ | 资料越相关，分数越高 |
| **召回率** | 该找到的资料都找到了吗？ | 遗漏越少，分数越高 |
| **排序质量** | 最有用的资料有没有排在前面？ | 排序越准，分数越高 |

#### 💬 回答能力评估（机器人答题的能力）

| 指标 | 通俗解释 | 评分标准 |
|------|----------|----------|
| **忠实度** | 回答是基于资料说的，还是自己瞎编的？ | 越忠实资料，分数越高 |
| **切题性** | 回答是否直接、聚焦，不跑题？ | 越切题，分数越高 |
| **正确性** | 和标准答案比，事实对不对？ | 越准确，分数越高 |

**评分示例**：

```
问题：我信用分 680，想买 40 万的房子，最低需要多少首付？

机器人回答：根据 FHA 贷款，信用分 680 可以享受 3.5% 的首付比例，
即 1.4 万美元。这是目前市场上最低的首付要求之一。

评分结果：
✅ 相关性：0.95 (A) - 找到的资料非常相关
⚠️ 召回率：0.75 (C) - 遗漏了信用分临界点的说明
✅ 排序质量：0.90 (A) - 最相关的资料排在前面
⚠️ 忠实度：0.80 (B) - "市场上最低"这句话不在资料里
✅ 切题性：0.92 (A) - 直接回答了问题
⚠️ 正确性：0.85 (B) - 主要信息正确，但信用分界限略有偏差

总分：0.86 (B)
```

### 第七步：生成体检报告 📄

Zeval 会生成一份详细的分析报告，包括：

#### 1. 总体评估（Executive Summary）

```
您的 AI 机器人在 50 个测试题中平均得分 0.78（B级）。

关键发现：
✅ 回答切题性表现优秀（0.89），用户体验较好
✅ 相关性较高（0.85），能找到正确的资料
⚠️ 忠实度偏低（0.68），存在一定的"胡编乱造"问题
⚠️ 召回率不足（0.70），经常遗漏重要信息

优先改进建议：
1. 【高优先级】优化生成模型，减少幻觉（提升忠实度）
2. 【高优先级】增加检索结果数量，避免遗漏（提升召回率）
3. 【中优先级】添加重排序模块，提升关键信息排序
```

#### 2. 问题案例深度分析

报告会列出得分最低的 5-10 个案例，逐一分析：

```
【严重问题】案例 #23 (总分: 0.32)

问题：VA 贷款的资格要求是什么？

机器人回答：VA 贷款是专为退伍军人设计的，无需首付，
利率通常比传统贷款低 0.5-1%，审批时间约 30-45 天。

标准答案：VA 贷款要求申请人为现役军人、退伍军人或其配偶，
需提供服役证明（DD214 表格），无最低信用分要求，但多数银行
要求 620 以上。

评分详情：
❌ 忠实度：0.20 - "审批时间 30-45 天"不在知识库中
❌ 正确性：0.40 - 遗漏了服役证明和信用分要求
⚠️ 召回率：0.50 - 没有检索到资格要求相关的段落

问题根源分析：
→ 检索时只找到了产品介绍部分，遗漏了资格要求部分
→ 生成时添加了不在资料里的"审批时间"信息

改进方向：
→ 优化检索关键词提取，"资格要求"应该匹配到正确段落
→ 强化 prompt，要求"仅基于提供的资料回答"
```

#### 3. 优秀案例展示

也会展示做得好的案例，帮你了解哪些场景表现最佳。

#### 4. Excel 数据表（可视化分析）

报告还包含 Excel 表格，用颜色标注分数高低：

- 🟢 **绿色**：表现优秀（A/B 级）
- 🟡 **黄色**：有待改进（C 级）
- 🔴 **红色**：严重问题（D/F 级）

可以按分数排序，快速找到问题案例。

---

## 🎁 Zeval 的核心价值

### 1. 节省时间和成本 ⏰💰

| 传统方式 | 使用 Zeval |
|---------|-----------|
| 1 周人工测试 | 1 小时自动化完成 |
| 50 个测试题 | 500 个测试题 |
| 主观评价，标准不一 | 客观评分，标准统一 |
| 难以重复测试 | 随时可以重跑 |

### 2. 更全面的测试覆盖 🎯

- ✅ 覆盖不同用户角色（新手、专家、特殊需求）
- ✅ 覆盖不同主题（产品、政策、流程）
- ✅ 避免重复题目（智能去重）
- ✅ 发现边角案例（人工测试容易忽略的场景）

### 3. 持续监控和改进 📈

每次更新知识库或优化模型后，重新跑一次 Zeval：

```
版本对比：
               v1.0    v1.1    改进幅度
忠实度         0.68 → 0.82    +20%  ✅
召回率         0.70 → 0.85    +21%  ✅
总体得分       0.78 → 0.86    +10%  ✅
```

### 4. 数据驱动的决策 📊

不再靠"感觉"，用数据说话：

- "这次优化后，忠实度提升了 20%，幻觉问题明显减少"
- "检索召回率还是不够，需要继续优化分块策略"
- "小白用户的满意度（切题性）很高，但专家用户的正确性还需提升"

---

## 🚀 典型应用场景

### 场景 1：智能客服上线前测试

**背景**：电商公司开发了智能客服，回答售后问题。

**流程**：
1. 上传《售后政策手册》《常见问题 FAQ》
2. Zeval 自动生成 300 个测试题
3. 机器人回答，Zeval 评分
4. 发现"退货流程"相关问题得分低（0.55）
5. 优化后重测，得分提升到 0.88
6. 通过测试，正式上线 ✅

### 场景 2：知识库更新后验证

**背景**：保险公司更新了 2025 年新政策。

**流程**：
1. 上传新旧两版政策文件
2. Zeval 生成针对新政策的测试题
3. 测试发现机器人还在引用旧政策（忠实度 0.45）
4. 更新知识库索引后重测
5. 得分恢复到 0.92，确认更新成功 ✅

### 场景 3：多个模型效果对比

**背景**：技术团队想选择最佳的 AI 模型。

**流程**：
1. 用相同的 100 道题测试 3 个模型
2. Zeval 生成对比报告：

| 模型 | 总分 | 忠实度 | 召回率 | 正确性 | 速度 |
|------|------|--------|--------|--------|------|
| A    | 0.82 | 0.85   | 0.78   | 0.80   | 快   |
| B    | 0.88 | 0.92   | 0.85   | 0.87   | 中   |
| C    | 0.79 | 0.75   | 0.82   | 0.78   | 慢   |

3. 决策：选择模型 B（综合表现最佳）✅

---

## 💡 和传统测试方法的对比

| 维度 | 传统人工测试 | Zeval 自动化测试 |
|------|-------------|-----------------|
| **出题** | 人工编写题目和答案 | 自动从知识库生成 |
| **执行** | 人工逐个提问记录 | 自动批量测试 |
| **评分** | 主观打分，标准不一 | 6 维度客观评分 |
| **速度** | 慢（几天到几周） | 快（几分钟到几小时） |
| **覆盖** | 有限（几十个题目） | 全面（几百个题目） |
| **重复性** | 难以完全重复 | 完全可重复 |
| **成本** | 高（持续人力投入） | 低（自动化运行） |
| **可追溯** | 依赖文档记录 | 完整的数据留存 |

---

## 🎓 总结：Zeval 的本质

如果用一个比喻来解释：

> **Zeval 就像是 AI 机器人的"自动体检仪"**

- 🏥 **体检中心** = 评估框架
- 📋 **体检项目** = 6 大评估指标
- 🩺 **体检报告** = 分析报告
- 💊 **治疗建议** = 改进建议

你不需要是医生，也能看懂体检报告；  
你不需要是工程师，也能用 Zeval 评估 AI 系统。

---

## 📚 下一步

- **想了解技术细节？** → 阅读 [`design_principles.md`](./design_principles.md)
- **想开始使用？** → 查看示例 `examples/test_e2e_complete.py`
- **想深入了解指标？** → 阅读 [`understanding_f1_score.md`](./understanding_f1_score.md)

---

*如果这份文档帮助你理解了 Zeval，欢迎分享给更多人！* 😊
