"""Evaluation reporter for generating analysis reports"""

import asyncio
import csv
from datetime import datetime
from pathlib import Path
from typing import Any
from pydantic import BaseModel, Field
from chak import Conversation

try:
    from openpyxl import Workbook
    from openpyxl.styles import PatternFill, Font, Alignment, Border, Side
    OPENPYXL_AVAILABLE = True
except ImportError:
    OPENPYXL_AVAILABLE = False

from ..schemas.eval import EvalDataset, EvalCase


class MetricStats(BaseModel):
    """Statistics for a single metric"""
    name: str
    avg_score: float
    min_score: float
    max_score: float
    std_dev: float
    count: int


class ExecutiveSummary(BaseModel):
    """High-level summary generated by LLM"""
    overall_assessment: str = Field(description="Overall performance assessment (1-2 sentences)")
    key_findings: list[str] = Field(description="3-5 key findings from the evaluation")
    priority_recommendations: list[str] = Field(description="Top 3-5 recommendations, ordered by priority")


class LLMAnalysis(BaseModel):
    """Complete LLM-generated analysis (simplified)"""
    executive_summary: ExecutiveSummary


class EvaluationReporter:
    """
    Reporter for generating evaluation analysis reports
    
    Generates detailed markdown reports with LLM-powered insights.
    Can load datasets from file or accept in-memory datasets.
    """
    
    METRIC_NAMES = [
        "faithfulness",
        "context_relevance", 
        "context_recall",
        "context_precision",
        "answer_relevancy",
        "answer_correctness"
    ]
    
    METRIC_DESCRIPTIONS = {
        "faithfulness": "答案是否基于检索到的上下文，没有幻觉",
        "context_relevance": "检索到的上下文是否与问题相关",
        "context_recall": "Ground truth 中的信息是否能从检索上下文中找到",
        "context_precision": "检索结果的排序质量，有用的上下文是否排在前面",
        "answer_relevancy": "答案是否切题、聚焦、实质性",
        "answer_correctness": "答案的事实正确性（对比 ground truth）",
    }
    
    def __init__(self, llm_uri: str, api_key: str):
        """
        Initialize reporter
        
        Args:
            llm_uri: LLM endpoint URI (e.g., 'bailian/qwen-plus')
            api_key: API key for LLM
        """
        self.llm_uri = llm_uri
        self.api_key = api_key
    
    async def generate_report(
        self,
        dataset: EvalDataset | None = None,
        dataset_path: str | None = None,
        output_path: str | None = None,
        export_csv: bool = True,
    ) -> str:
        """
        Generate evaluation report
        
        Args:
            dataset: In-memory dataset (if already available)
            dataset_path: Path to load dataset from JSON (if not in memory)
            output_path: Path to save markdown report (optional)
            export_csv: Whether to export CSV files (default: True)
            
        Returns:
            Markdown report content
        """
        # Load dataset if needed
        if dataset is None:
            if dataset_path is None:
                raise ValueError("Either dataset or dataset_path must be provided")
            dataset = EvalDataset.from_json(dataset_path)
        
        # Collect statistics
        stats = self._collect_statistics(dataset)
        
        # Identify bad cases and good cases
        bad_cases = self._identify_bad_cases(dataset)
        good_cases = self._identify_good_cases(dataset)
        
        # Generate LLM analysis
        llm_analysis = await self._generate_llm_analysis(dataset, stats, bad_cases)
        
        # Generate markdown report
        report = self._generate_markdown_report(
            dataset=dataset,
            stats=stats,
            bad_cases=bad_cases,
            good_cases=good_cases,
            llm_analysis=llm_analysis
        )
        
        # Save to file if requested
        if output_path:
            output_file = Path(output_path)
            
            # Create timestamped directory
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_dir = output_file.parent / f"report_{timestamp}"
            report_dir.mkdir(parents=True, exist_ok=True)
            
            # Save markdown report in timestamped directory
            report_file = report_dir / output_file.name
            report_file.write_text(report, encoding='utf-8')
            
            print(f"Report saved to: {report_file}")
            
            # Export Excel/CSV files to the same timestamped directory
            if export_csv:
                if OPENPYXL_AVAILABLE:
                    # Classify cases for query type analysis
                    classification = self._classify_cases_by_query_type(dataset)
                    stats_by_type = self._compute_stats_by_query_type(classification)
                    self._export_excel(dataset, stats, bad_cases, report_dir, stats_by_type)
                else:
                    print("[INFO] openpyxl not available, falling back to CSV export")
                    self._export_csv(dataset, stats, bad_cases, report_dir)
        
        return report
    
    def _collect_statistics(self, dataset: EvalDataset) -> dict[str, MetricStats]:
        """Collect statistics for each metric"""
        stats = {}
        
        for metric_name in self.METRIC_NAMES:
            scores = []
            for case in dataset.cases:
                result = case.results.get(metric_name)
                if result:
                    scores.append(result.score)
            
            if scores:
                avg = sum(scores) / len(scores)
                variance = sum((s - avg) ** 2 for s in scores) / len(scores)
                std_dev = variance ** 0.5
                
                stats[metric_name] = MetricStats(
                    name=metric_name,
                    avg_score=avg,
                    min_score=min(scores),
                    max_score=max(scores),
                    std_dev=std_dev,
                    count=len(scores)
                )
        
        return stats
    
    def _classify_cases_by_query_type(self, dataset: EvalDataset) -> dict[str, list[EvalCase]]:
        """Classify cases by query type (based on source_units count)"""
        classification = {
            "single_hop": [],
            "multi_hop_2": [],
            "multi_hop_3+": []
        }
        
        for case in dataset.cases:
            unit_count = len(case.source_units)
            if unit_count == 1:
                classification["single_hop"].append(case)
            elif unit_count == 2:
                classification["multi_hop_2"].append(case)
            else:
                classification["multi_hop_3+"].append(case)
        
        return classification
    
    def _compute_stats_by_query_type(
        self,
        classification: dict[str, list[EvalCase]]
    ) -> dict[str, dict[str, float]]:
        """Compute statistics for each query type"""
        stats_by_type = {}
        
        for query_type, cases in classification.items():
            if not cases:
                continue
            
            type_stats = {
                "count": len(cases),
                "avg_overall_score": sum(c.overall_score for c in cases) / len(cases)
            }
            
            # Compute average for each metric
            for metric_name in self.METRIC_NAMES:
                scores = []
                for case in cases:
                    result = case.results.get(metric_name)
                    if result:
                        scores.append(result.score)
                
                if scores:
                    type_stats[metric_name] = sum(scores) / len(scores)
                else:
                    type_stats[metric_name] = 0.0
            
            stats_by_type[query_type] = type_stats
        
        return stats_by_type
    
    def _identify_bad_cases(self, dataset: EvalDataset, threshold: float = 0.5) -> list[tuple[int, EvalCase, float]]:
        """
        Identify problematic cases
        
        Returns:
            List of (case_index, case, overall_score) tuples, sorted by score (worst first)
        """
        bad_cases = []
        
        for i, case in enumerate(dataset.cases):
            overall_score = case.overall_score
            if overall_score < threshold:
                bad_cases.append((i, case, overall_score))
        
        # Sort by score (worst first)
        bad_cases.sort(key=lambda x: x[2])
        
        return bad_cases
    
    def _identify_good_cases(self, dataset: EvalDataset, threshold: float = 0.8) -> list[tuple[int, EvalCase, float]]:
        """
        Identify excellent cases
        
        Returns:
            List of (case_index, case, overall_score) tuples, sorted by score (best first)
        """
        good_cases = []
        
        for i, case in enumerate(dataset.cases):
            overall_score = case.overall_score
            if overall_score >= threshold:
                good_cases.append((i, case, overall_score))
        
        # Sort by score (best first)
        good_cases.sort(key=lambda x: x[2], reverse=True)
        
        return good_cases
    
    async def _generate_llm_analysis(
        self,
        dataset: EvalDataset,
        stats: dict[str, MetricStats],
        bad_cases: list[tuple[int, EvalCase, float]]
    ) -> LLMAnalysis:
        """Use LLM to generate high-level insights (simplified)"""
        
        # Prepare concise stats summary for LLM
        stats_summary = "\n".join([
            f"- {s.name}: 平均 {s.avg_score:.2f} ({self._get_grade(s.avg_score)})"
            for s in stats.values()
        ])
        
        bad_case_count = len(bad_cases)
        
        prompt = f"""你是 RAG 系统评估专家。请简洁分析以下评估结果。

## 评估概况
总共评估 {len(dataset.cases)} 个案例，各指标表现：

{stats_summary}

问题案例数：{bad_case_count} 个（总分 < 0.5）

请提供：
1. 总体评估（1-2句话，说明整体表现水平）
2. 关键发现（3-5条，指出最重要的问题和优势）
3. 优先改进建议（3-5条，按优先级排序，说明先改什么）

要求：
- 基于数据客观分析，不要泛泛而谈
- 建议要具体可执行
- 用中文回答
"""
        
        # Create new conversation for each call to avoid state pollution
        conv = Conversation(model_uri=self.llm_uri, api_key=self.api_key)
        
        try:
            # Increase timeout for complex analysis
            analysis = await asyncio.wait_for(
                conv.asend(prompt, returns=LLMAnalysis),
                timeout=60.0  # 1 minute should be enough for simplified prompt
            )
            return analysis
        except asyncio.TimeoutError:
            print(f"[WARNING] LLM analysis timeout after 60s, using fallback")
            return self._generate_fallback_analysis(stats, bad_cases)
        except Exception as e:
            # Fallback to basic analysis if LLM fails
            print(f"[WARNING] LLM analysis failed: {e}, using fallback")
            return self._generate_fallback_analysis(stats, bad_cases)
    
    def _generate_fallback_analysis(
        self,
        stats: dict[str, MetricStats],
        bad_cases: list[tuple[int, EvalCase, float]]
    ) -> LLMAnalysis:
        """Generate basic analysis without LLM (simplified)"""
        
        # Find worst and best metrics
        sorted_metrics = sorted(stats.values(), key=lambda s: s.avg_score)
        worst_metrics = sorted_metrics[:2]  # Bottom 2
        best_metrics = sorted_metrics[-2:]  # Top 2
        
        # Generate findings
        key_findings = []
        for m in worst_metrics:
            if m.avg_score < 0.7:
                key_findings.append(f"{m.name} 表现较差 (平均 {m.avg_score:.2f})")
        
        for m in reversed(best_metrics):
            if m.avg_score >= 0.8:
                key_findings.append(f"{m.name} 表现良好 (平均 {m.avg_score:.2f})")
        
        if bad_cases:
            key_findings.append(f"发现 {len(bad_cases)} 个问题案例，需要重点调查")
        
        # Generate recommendations
        recommendations = []
        for m in worst_metrics:
            if m.avg_score < 0.7:
                recommendations.append(f"优先改进 {m.name}，当前得分较低 ({m.avg_score:.2f})")
        
        return LLMAnalysis(
            executive_summary=ExecutiveSummary(
                overall_assessment=f"评估了 {sum(s.count for s in stats.values())} 个案例，整体表现中等。",
                key_findings=key_findings if key_findings else ["所有指标表现均衡"],
                priority_recommendations=recommendations if recommendations else ["保持当前水平"]
            )
        )
    
    def _generate_markdown_report(
        self,
        dataset: EvalDataset,
        stats: dict[str, MetricStats],
        bad_cases: list[tuple[int, EvalCase, float]],
        good_cases: list[tuple[int, EvalCase, float]],
        llm_analysis: LLMAnalysis
    ) -> str:
        """Generate markdown report"""
        
        lines = []
        
        # Header
        lines.append("# RAG Evaluation Report")
        lines.append("")
        lines.append(f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"**Total Cases**: {len(dataset.cases)}")
        lines.append(f"**Metrics Evaluated**: {len(stats)}")
        lines.append("")
        lines.append("---")
        lines.append("")
        
        # Executive Summary
        lines.append("## Executive Summary")
        lines.append("")
        lines.append(f"> {llm_analysis.executive_summary.overall_assessment}")
        lines.append("")
        
        if llm_analysis.executive_summary.key_findings:
            lines.append("### Key Findings")
            lines.append("")
            for finding in llm_analysis.executive_summary.key_findings:
                lines.append(f"- {finding}")
            lines.append("")
        
        if llm_analysis.executive_summary.priority_recommendations:
            lines.append("### Priority Recommendations")
            lines.append("")
            for i, rec in enumerate(llm_analysis.executive_summary.priority_recommendations, 1):
                lines.append(f"{i}. {rec}")
            lines.append("")
        
        lines.append("---")
        lines.append("")
                
        # Metrics Overview
        lines.append("## Metrics Overview")
        lines.append("")
        lines.append("| Metric | Avg Score | Min | Max | Std Dev | Grade |")
        lines.append("|--------|-----------|-----|-----|---------|-------|")
                
        for metric_name in self.METRIC_NAMES:
            if metric_name in stats:
                s = stats[metric_name]
                grade = self._get_grade(s.avg_score)
                lines.append(
                    f"| {metric_name} | {s.avg_score:.3f} | {s.min_score:.2f} | "
                    f"{s.max_score:.2f} | {s.std_dev:.3f} | {grade} |"
                )
                
        lines.append("")
        lines.append("**Grading Scale**: A (≥ 0.9), B (≥ 0.8), C (≥ 0.7), D (≥ 0.6), F (< 0.6)")
        lines.append("")
        lines.append("---")
        lines.append("")
                
        # Metrics Explanation (Fixed)
        lines.append("## Metric Details")
        lines.append("")
                
        for metric_name in self.METRIC_NAMES:
            if metric_name not in stats:
                continue
                        
            metric_desc = self.METRIC_DESCRIPTIONS.get(metric_name, "")
            metric_stat = stats[metric_name]
                    
            lines.append(f"### {metric_name}")
            lines.append("")
            lines.append(f"**Definition**: {metric_desc}")
            lines.append("")
            lines.append(f"**Current Score**: {metric_stat.avg_score:.2f} ({self._get_grade(metric_stat.avg_score)})")
            lines.append("")
                    
            # Add fixed improvement suggestions based on metric type
            lines.append("**Improvement Guidelines**:")
            if metric_name == "faithfulness":
                lines.append("- Low score: Check if generation model produces hallucinations or over-inferences")
                lines.append("- Optimize prompt to emphasize 'answer only based on provided context'")
                lines.append("- Verify retrieval results are fully passed to generation model")
            elif metric_name == "context_relevance":
                lines.append("- Low score: Retrieval system returns irrelevant content")
                lines.append("- Improve retrieval query and semantic matching between query and documents")
                lines.append("- Consider adding reranking step")
            elif metric_name == "context_recall":
                lines.append("- Low score: Retrieval system fails to find required information")
                lines.append("- Increase retrieval result count (top-k)")
                lines.append("- Check if knowledge base contains relevant content")
                lines.append("- Optimize chunking strategy to prevent splitting of key information")
            elif metric_name == "context_precision":
                lines.append("- Low score: Useful contexts are ranked low")
                lines.append("- Add reranking model")
                lines.append("- Adjust retrieval algorithm parameters to improve ranking quality")
            elif metric_name == "answer_relevancy":
                lines.append("- Low score: Answer is off-topic or verbose")
                lines.append("- Optimize prompt to require direct answers")
                lines.append("- Check if question understanding module works properly")
            elif metric_name == "answer_correctness":
                lines.append("- Low score: Answer contains incorrect or incomplete information")
                lines.append("- Check retrieval quality (context_recall)")
                lines.append("- Check if generation model misinterprets context (faithfulness)")
                lines.append("- Optimize generation prompt to ensure information completeness")
                    
            lines.append("")
        
        lines.append("---")
        lines.append("")
        
        # Bad Cases
        if bad_cases:
            lines.append(f"## Problem Cases ({len(bad_cases)} cases)")
            lines.append("")
            
            for idx, case, overall_score in bad_cases[:5]:  # Show top 5 worst
                severity = "[CRITICAL]" if overall_score < 0.3 else "[WARNING]"
                lines.append(f"### {severity} Case #{idx+1} (Score: {overall_score:.2f})")
                lines.append("")
                
                # Question and Answer
                lines.append(f"**Question**: {case.question}")
                lines.append("")
                
                answer_preview = case.answer[:200] + "..." if case.answer and len(case.answer) > 200 else (case.answer or "N/A")
                lines.append(f"**Answer**: {answer_preview}")
                lines.append("")
                
                gt_preview = case.ground_truth_answer[:200] + "..." if len(case.ground_truth_answer) > 200 else case.ground_truth_answer
                lines.append(f"**Ground Truth**: {gt_preview}")
                lines.append("")
                
                # Retrieved Contexts
                if case.retrieved_contexts:
                    lines.append("**Retrieved Contexts**:")
                    for i, ctx in enumerate(case.retrieved_contexts[:3], 1):  # Show top 3
                        ctx_preview = ctx[:150] + "..." if len(ctx) > 150 else ctx
                        lines.append(f"{i}. {ctx_preview}")
                    lines.append("")
                
                # Metric scores with details
                lines.append("**Metric Scores**:")
                for metric_name in self.METRIC_NAMES:
                    result = case.results.get(metric_name)
                    if result:
                        status = "[FAIL]" if result.score < 0.5 else "[WARN]" if result.score < 0.8 else "[PASS]"
                        reason_preview = result.reason[:100] + "..." if result.reason and len(result.reason) > 100 else (result.reason or "")
                        lines.append(f"- {status} **{metric_name}**: {result.score:.2f} - {reason_preview}")
                lines.append("")
                
                # Root cause analysis
                worst_metric_name = min(
                    [m for m in self.METRIC_NAMES if m in case.results],
                    key=lambda m: case.results[m].score,
                    default=None
                )
                if worst_metric_name:
                    lines.append(f"**Investigation Focus**: Review {worst_metric_name} metric (see Metric Details section above)")
                    lines.append("")
                
                lines.append("---")
                lines.append("")
        
        # Good Cases
        if good_cases:
            lines.append(f"## Excellent Cases ({len(good_cases)} cases)")
            lines.append("")
            
            for idx, case, overall_score in good_cases[:3]:  # Show top 3 best
                lines.append(f"### Case #{idx+1} (Score: {overall_score:.2f})")
                lines.append("")
                lines.append(f"**Question**: {case.question}")
                lines.append("")
                
                # Show metrics
                lines.append("**Metric Scores**:")
                for metric_name in self.METRIC_NAMES:
                    result = case.results.get(metric_name)
                    if result:
                        lines.append(f"- {metric_name}: {result.score:.2f}")
                lines.append("")
        
        # Footer
        lines.append("---")
        lines.append("")
        lines.append("*Report generated by zeval-ai evaluation system*")
        lines.append("")
        
        return "\n".join(lines)
    
    def _get_grade(self, score: float) -> str:
        """Convert score to letter grade"""
        if score >= 0.9:
            return "A"
        elif score >= 0.8:
            return "B"
        elif score >= 0.7:
            return "C"
        elif score >= 0.6:
            return "D"
        else:
            return "F"
    
    def _get_status(self, score: float) -> str:
        """Get status label for score"""
        if score >= 0.8:
            return "GOOD"
        elif score >= 0.7:
            return "FAIR"
        elif score >= 0.6:
            return "NEEDS_IMPROVEMENT"
        else:
            return "CRITICAL"
    
    def _export_csv(
        self,
        dataset: EvalDataset,
        stats: dict[str, MetricStats],
        bad_cases: list[tuple[int, EvalCase, float]],
        output_dir: Path
    ):
        """Export evaluation data to CSV files"""
        
        # 1. Export metrics summary
        metrics_summary_path = output_dir / "metrics_summary.csv"
        with open(metrics_summary_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['Metric', 'Avg_Score', 'Min', 'Max', 'Std_Dev', 'Grade', 'Status'])
            
            for metric_name in self.METRIC_NAMES:
                if metric_name in stats:
                    s = stats[metric_name]
                    writer.writerow([
                        metric_name,
                        f"{s.avg_score:.3f}",
                        f"{s.min_score:.2f}",
                        f"{s.max_score:.2f}",
                        f"{s.std_dev:.3f}",
                        self._get_grade(s.avg_score),
                        self._get_status(s.avg_score)
                    ])
        
        # 2. Export case details
        case_details_path = output_dir / "case_details.csv"
        with open(case_details_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # Header: Case_ID, Overall_Score, Question, Answer_Preview, [all metrics], Status
            header = ['Case_ID', 'Overall_Score', 'Question', 'Answer_Preview'] + self.METRIC_NAMES + ['Status']
            writer.writerow(header)
            
            for i, case in enumerate(dataset.cases, 1):
                overall_score = case.overall_score
                
                # Status based on overall score
                if overall_score >= 0.8:
                    status = "EXCELLENT"
                elif overall_score >= 0.5:
                    status = "GOOD"
                elif overall_score >= 0.3:
                    status = "WARNING"
                else:
                    status = "CRITICAL"
                
                # Answer preview (first 100 chars)
                answer_preview = (case.answer[:100] + "...") if case.answer and len(case.answer) > 100 else (case.answer or "N/A")
                
                # Build row
                row = [
                    i,
                    f"{overall_score:.2f}",
                    case.question,
                    answer_preview
                ]
                
                # Add each metric score
                for metric_name in self.METRIC_NAMES:
                    result = case.results.get(metric_name)
                    if result:
                        row.append(f"{result.score:.2f}")
                    else:
                        row.append("N/A")
                
                row.append(status)
                
                writer.writerow(row)
        
        print(f"CSV files exported to:")
        print(f"  - {metrics_summary_path}")
        print(f"  - {case_details_path}")
    
    def _export_excel(
        self,
        dataset: EvalDataset,
        stats: dict[str, MetricStats],
        bad_cases: list[tuple[int, EvalCase, float]],
        output_dir: Path,
        stats_by_type: dict[str, dict[str, float]] | None = None
    ):
        """Export evaluation data to Excel with color formatting"""
        
        # Create workbook with two sheets
        wb = Workbook()
        
        # Define border style
        thin_border = Border(
            left=Side(style='thin'),
            right=Side(style='thin'),
            top=Side(style='thin'),
            bottom=Side(style='thin')
        )
        
        # Sheet 1: Metrics Summary
        ws_metrics = wb.active
        ws_metrics.title = "Metrics Summary"
        
        # Header
        headers = ['Metric', 'Avg Score', 'Min', 'Max', 'Std Dev', 'Grade', 'Status']
        ws_metrics.append(headers)
        
        # Style header
        for cell in ws_metrics[1]:
            cell.font = Font(bold=True, color="FFFFFF")
            cell.fill = PatternFill(start_color="424242", end_color="424242", fill_type="solid")
            cell.alignment = Alignment(horizontal="center")
            cell.border = thin_border
        
        # Add data with colors
        for metric_name in self.METRIC_NAMES:
            if metric_name in stats:
                s = stats[metric_name]
                grade = self._get_grade(s.avg_score)
                status = self._get_status(s.avg_score)
                
                row = [
                    metric_name,
                    round(s.avg_score, 3),
                    round(s.min_score, 2),
                    round(s.max_score, 2),
                    round(s.std_dev, 3),
                    grade,
                    status
                ]
                ws_metrics.append(row)
                
                # Color the entire row based on score
                row_idx = ws_metrics.max_row
                fill_color = self._get_color_for_score(s.avg_score)
                for cell in ws_metrics[row_idx]:
                    cell.fill = PatternFill(start_color=fill_color, end_color=fill_color, fill_type="solid")
                    cell.font = Font(color="FFFFFF", bold=True)  # All colored cells use white text
                    cell.border = thin_border
        
        # Adjust column widths
        ws_metrics.column_dimensions['A'].width = 20
        ws_metrics.column_dimensions['B'].width = 12
        ws_metrics.column_dimensions['C'].width = 8
        ws_metrics.column_dimensions['D'].width = 8
        ws_metrics.column_dimensions['E'].width = 10
        ws_metrics.column_dimensions['F'].width = 8
        ws_metrics.column_dimensions['G'].width = 20
        
        # Sheet 2: Performance by Query Type (if available)
        if stats_by_type:
            ws_query_type = wb.create_sheet("Performance by Query Type")
            
            # Header
            headers = ['Query Type', 'Count', 'Avg Overall Score'] + self.METRIC_NAMES
            ws_query_type.append(headers)
            
            # Style header
            for cell in ws_query_type[1]:
                cell.font = Font(bold=True, color="FFFFFF")
                cell.fill = PatternFill(start_color="424242", end_color="424242", fill_type="solid")
                cell.alignment = Alignment(horizontal="center")
                cell.border = thin_border
            
            # Define query type display names
            type_display_names = {
                "single_hop": "Single-Hop (1 unit)",
                "multi_hop_2": "Multi-Hop (2 units)",
                "multi_hop_3+": "Multi-Hop (3+ units)"
            }
            
            # Add data
            for query_type in ["single_hop", "multi_hop_2", "multi_hop_3+"]:
                if query_type not in stats_by_type:
                    continue
                
                type_stats = stats_by_type[query_type]
                row = [
                    type_display_names[query_type],
                    int(type_stats["count"]),
                    round(type_stats["avg_overall_score"], 3)
                ]
                
                # Add metric scores
                for metric_name in self.METRIC_NAMES:
                    row.append(round(type_stats.get(metric_name, 0.0), 3))
                
                ws_query_type.append(row)
                
                # Color the Avg Overall Score cell
                row_idx = ws_query_type.max_row
                avg_score_cell = ws_query_type.cell(row=row_idx, column=3)
                fill_color = self._get_color_for_score(type_stats["avg_overall_score"])
                avg_score_cell.fill = PatternFill(start_color=fill_color, end_color=fill_color, fill_type="solid")
                avg_score_cell.font = Font(color="FFFFFF", bold=True)
                avg_score_cell.border = thin_border
                
                # Color individual metric cells
                for col_idx, metric_name in enumerate(self.METRIC_NAMES, start=4):
                    cell = ws_query_type.cell(row=row_idx, column=col_idx)
                    score = type_stats.get(metric_name, 0.0)
                    fill_color = self._get_color_for_score(score)
                    cell.fill = PatternFill(start_color=fill_color, end_color=fill_color, fill_type="solid")
                    cell.font = Font(color="FFFFFF", bold=True)
                    cell.border = thin_border
                
                # Add borders to other cells
                for col_idx in [1, 2]:
                    cell = ws_query_type.cell(row=row_idx, column=col_idx)
                    cell.border = thin_border
            
            # Adjust column widths
            ws_query_type.column_dimensions['A'].width = 25  # Query Type
            ws_query_type.column_dimensions['B'].width = 10  # Count
            ws_query_type.column_dimensions['C'].width = 18  # Avg Overall Score
            for i in range(len(self.METRIC_NAMES)):
                ws_query_type.column_dimensions[chr(68 + i)].width = 15  # Metric columns
        
        # Sheet 3: Case Details
        ws_cases = wb.create_sheet("Case Details")
        
        # Header: scores first (with colors), then details
        headers = [
            'Case ID',
            'Overall Score',
            'Status'
        ] + self.METRIC_NAMES + [
            'Question',
            'Answer',
            'Ground Truth Answer',
            'Retrieved Contexts',
            'Ground Truth Contexts'
        ]
        ws_cases.append(headers)
        
        # Style header
        for cell in ws_cases[1]:
            cell.font = Font(bold=True, color="FFFFFF")
            cell.fill = PatternFill(start_color="424242", end_color="424242", fill_type="solid")
            cell.alignment = Alignment(horizontal="center")
            cell.border = thin_border
        
        # Add data with colors
        for i, case in enumerate(dataset.cases, 1):
            overall_score = case.overall_score
            
            # Status
            if overall_score >= 0.8:
                status = "EXCELLENT"
            elif overall_score >= 0.5:
                status = "GOOD"
            elif overall_score >= 0.3:
                status = "WARNING"
            else:
                status = "CRITICAL"
            
            # Format contexts as numbered list
            retrieved_contexts_text = "\n".join([
                f"{idx+1}. {ctx}" for idx, ctx in enumerate(case.retrieved_contexts)
            ]) if case.retrieved_contexts else "N/A"
            
            ground_truth_contexts_text = "\n".join([
                f"{idx+1}. {ctx}" for idx, ctx in enumerate(case.ground_truth_contexts)
            ])
            
            # Build row: scores first, then details
            row = [
                i,
                round(overall_score, 2),
                status
            ]
            
            # Add all metric scores
            for metric_name in self.METRIC_NAMES:
                result = case.results.get(metric_name)
                if result:
                    row.append(round(result.score, 2))
                else:
                    row.append("N/A")
            
            # Add detailed information
            row.extend([
                case.question,
                case.answer or "N/A",
                case.ground_truth_answer,
                retrieved_contexts_text,
                ground_truth_contexts_text
            ])
            
            ws_cases.append(row)
            
            # Color the Overall Score cell (column B)
            row_idx = ws_cases.max_row
            overall_score_cell = ws_cases.cell(row=row_idx, column=2)
            fill_color = self._get_color_for_score(overall_score)
            overall_score_cell.fill = PatternFill(start_color=fill_color, end_color=fill_color, fill_type="solid")
            overall_score_cell.font = Font(color="FFFFFF", bold=True)  # White text for all colored cells
            overall_score_cell.border = thin_border
            
            # Color individual metric cells (columns D onwards, after Case ID, Overall Score, Status)
            for col_idx, metric_name in enumerate(self.METRIC_NAMES, start=4):
                result = case.results.get(metric_name)
                if result:
                    cell = ws_cases.cell(row=row_idx, column=col_idx)
                    fill_color = self._get_color_for_score(result.score)
                    cell.fill = PatternFill(start_color=fill_color, end_color=fill_color, fill_type="solid")
                    cell.font = Font(color="FFFFFF", bold=True)  # White text for all colored cells
                    cell.border = thin_border
            
            # Add borders to non-colored cells
            for col_idx in [1, 3] + list(range(4 + len(self.METRIC_NAMES), 4 + len(self.METRIC_NAMES) + 5)):
                cell = ws_cases.cell(row=row_idx, column=col_idx)
                cell.border = thin_border
            
            # Enable text wrapping for long text fields
            for col in [row_idx]:
                for text_col in range(4 + len(self.METRIC_NAMES), 4 + len(self.METRIC_NAMES) + 5):
                    cell = ws_cases.cell(row=row_idx, column=text_col)
                    cell.alignment = Alignment(wrap_text=True, vertical='top')
        
        # Adjust column widths
        ws_cases.column_dimensions['A'].width = 10  # Case ID
        ws_cases.column_dimensions['B'].width = 15  # Overall Score
        ws_cases.column_dimensions['C'].width = 12  # Status
        
        # Metric columns
        for i in range(len(self.METRIC_NAMES)):
            ws_cases.column_dimensions[chr(68 + i)].width = 15  # D, E, F, ...
        
        # Detail columns (after metrics)
        detail_col_start = 68 + len(self.METRIC_NAMES)  # ASCII code
        ws_cases.column_dimensions[chr(detail_col_start)].width = 60      # Question
        ws_cases.column_dimensions[chr(detail_col_start + 1)].width = 60  # Answer
        ws_cases.column_dimensions[chr(detail_col_start + 2)].width = 60  # Ground Truth Answer
        ws_cases.column_dimensions[chr(detail_col_start + 3)].width = 80  # Retrieved Contexts
        ws_cases.column_dimensions[chr(detail_col_start + 4)].width = 80  # Ground Truth Contexts
        
        # Save workbook
        excel_path = output_dir / "evaluation_report.xlsx"
        wb.save(excel_path)
        
        print(f"Excel file exported to: {excel_path}")
    
    def _get_color_for_score(self, score: float) -> str:
        """Get background color hex for score"""
        if score >= 0.9:
            return "4CAF50"  # Green (A)
        elif score >= 0.8:
            return "8BC34A"  # Light Green (B)
        elif score >= 0.7:
            return "FFC107"  # Amber (C)
        elif score >= 0.6:
            return "FF9800"  # Orange (D)
        else:
            return "F44336"  # Red (F)
